{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Funzione per estrarre e convertire i valori dell'ottava colonna in liste di float\n",
    "def get_eight_column_values(df, type_val, module_val, name_val):\n",
    "    filtered_df = df[(df['type'] == type_val) & (df['module'] == module_val) & (df['name'] == name_val)]\n",
    "    if filtered_df.empty:\n",
    "        return None\n",
    "    values = filtered_df.iloc[:, 7].str.split().apply(lambda x: list(map(float, x)))\n",
    "    return values.explode().astype(float).values\n",
    "\n",
    "# Funzione per estrarre e convertire i valori della settima colonna in liste di float\n",
    "def get_seventh_column_values(df, type_val, module_val, name_val):\n",
    "    filtered_df = df[(df['type'] == type_val) & (df['module'] == module_val) & (df['name'] == name_val)]\n",
    "    if filtered_df.empty:\n",
    "        return None\n",
    "    values = filtered_df.iloc[:, 6].str.split().apply(lambda x: list(map(float, x)))\n",
    "    return values.explode().astype(float).values\n",
    "\n",
    "# Funzione per calcolare l'intervallo di confidenza t-student\n",
    "def t_student_confidence_interval(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    sem = stats.sem(data)  # Standard error of the mean\n",
    "    margin = sem * stats.t.ppf((1 + confidence) / 2, len(data) - 1)\n",
    "    return mean, mean - margin, mean + margin\n",
    "\n",
    "# Funzione per calcolare l'intervallo di confidenza naive\n",
    "def naive_confidence_interval(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    margin = mean * (1 - confidence)\n",
    "    return mean, mean - margin, mean + margin\n",
    "\n",
    "# Funzione per calcolare l'intervallo di confidenza normale\n",
    "def normal_confidence_interval(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    std_error = stats.sem(data)  # Standard error of the mean\n",
    "    z_value = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
    "    margin = z_value * std_error\n",
    "    return mean, mean - margin, mean + margin\n",
    "\n",
    "# Funzione per rimuovere '_vec' dalla fine del nome del file\n",
    "def remove_vec_suffix(file_name):\n",
    "    if file_name.endswith('_vec.csv'):\n",
    "        return file_name[:-8]\n",
    "    else:\n",
    "        return file_name\n",
    "\n",
    "# Funzione per ottenere il numero finale dal nome del file\n",
    "def get_file_number(file_name):\n",
    "    match = re.search(r'\\d+$', file_name)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return 0  # Se non viene trovato nessun numero, restituisci 0\n",
    "\n",
    "# Funzione per analizzare un singolo file CSV\n",
    "def analyze_csv(file_path, ignored_files, results):\n",
    "    df = pd.read_csv(file_path)\n",
    "    description = df[df['attrname'] == 'description']['attrvalue'].values\n",
    "    description = description[0] if len(description) > 0 else \"N/A\"\n",
    "\n",
    "    is_N = any(\"N=1\" in val for val in df[df['attrname'] == 'description']['attrvalue'].values)\n",
    "\n",
    "    pDistribution = get_eight_column_values(df, \"vector\", \"TandemQueueSystem.Server\", \"pDistribution\")\n",
    "    vDistribution = get_eight_column_values(df, \"vector\", \"TandemQueueSystem.Server\", \"vDistribution\")\n",
    "    lifeTime = get_eight_column_values(df, \"vector\", \"TandemQueueSystem.sink\", \"lifeTime:vector\")\n",
    "    lifeTime_arrival = get_seventh_column_values(df, \"vector\", \"TandemQueueSystem.sink\", \"lifeTime:vector\")\n",
    "    lifeTime_change_Q1 = get_seventh_column_values(df, \"vector\", \"TandemQueueSystem.Q1\", \"queueLength:vector\")\n",
    "    lifeTime_change_Q2 = get_seventh_column_values(df, \"vector\", \"TandemQueueSystem.Q2\", \"queueLength:vector\")\n",
    "\n",
    "    if pDistribution is None or vDistribution is None or lifeTime is None:\n",
    "        ignored_files.append((remove_vec_suffix(os.path.basename(file_path)), description))\n",
    "        return\n",
    "\n",
    "    min_length = min(len(pDistribution), len(vDistribution), len(lifeTime))\n",
    "    pDistribution = pDistribution[:min_length]\n",
    "    vDistribution = vDistribution[:min_length]\n",
    "    lifeTime = lifeTime[:min_length]\n",
    "\n",
    "    queueLength_Q1 = get_eight_column_values(df, \"vector\", \"TandemQueueSystem.Q1\", \"queueLength:vector\")\n",
    "    queueLength_Q2 = get_eight_column_values(df, \"vector\", \"TandemQueueSystem.Q2\", \"queueLength:vector\")\n",
    "\n",
    "    if is_N:\n",
    "        queueLength_Q2 = np.zeros(len(queueLength_Q1))\n",
    "    else: \n",
    "        if queueLength_Q1 is None or queueLength_Q2 is None:\n",
    "            ignored_files.append((remove_vec_suffix(os.path.basename(file_path)), description))\n",
    "            return\n",
    "\n",
    "    Cw = 1  # Definisci il valore di Cw\n",
    "\n",
    "    conf_levels = [0.95, 0.90, 0.85, 0.80]\n",
    "    lifeTime_stats_t_student = {}\n",
    "    U_values_stats_t_student = {}\n",
    "    utilization_Q1_stats_t_student = {}\n",
    "    utilization_Q2_stats_t_student = {}\n",
    "\n",
    "    lifeTime_stats_naive = {}\n",
    "    U_values_stats_naive = {}\n",
    "    utilization_Q1_stats_naive = {}\n",
    "    utilization_Q2_stats_naive = {}\n",
    "    \n",
    "    lifeTime_stats_normal = {}\n",
    "    U_values_stats_normal = {}\n",
    "    utilization_Q1_stats_normal = {}\n",
    "    utilization_Q2_stats_normal = {}\n",
    "\n",
    "    for conf in conf_levels:\n",
    "        # Calcoli t-student\n",
    "        mean_W_t, ci_low_W_t, ci_high_W_t = t_student_confidence_interval(lifeTime, confidence=conf)\n",
    "        lifeTime_stats_t_student[conf] = (mean_W_t, ci_low_W_t, ci_high_W_t)\n",
    "        \n",
    "        U_values = vDistribution - pDistribution - (Cw * lifeTime)\n",
    "        mean_U_t, ci_low_U_t, ci_high_U_t = t_student_confidence_interval(U_values, confidence=conf)\n",
    "        U_values_stats_t_student[conf] = (mean_U_t, ci_low_U_t, ci_high_U_t)\n",
    "\n",
    "        mean_utilization_Q1_t, ci_low_utilization_Q1_t, ci_high_utilization_Q1_t = t_student_confidence_interval(queueLength_Q1 / np.sum(queueLength_Q1), confidence=conf)\n",
    "        utilization_Q1_stats_t_student[conf] = (mean_utilization_Q1_t, ci_low_utilization_Q1_t, ci_high_utilization_Q1_t)\n",
    "\n",
    "        mean_utilization_Q2_t, ci_low_utilization_Q2_t, ci_high_utilization_Q2_t = t_student_confidence_interval(queueLength_Q2 / np.sum(queueLength_Q2), confidence=conf)\n",
    "        utilization_Q2_stats_t_student[conf] = (mean_utilization_Q2_t, ci_low_utilization_Q2_t, ci_high_utilization_Q2_t)\n",
    "\n",
    "        # Calcoli naive\n",
    "        mean_W_n, ci_low_W_n, ci_high_W_n = naive_confidence_interval(lifeTime, confidence=conf)\n",
    "        lifeTime_stats_naive[conf] = (mean_W_n, ci_low_W_n, ci_high_W_n)\n",
    "        \n",
    "        mean_U_n, ci_low_U_n, ci_high_U_n = naive_confidence_interval(U_values, confidence=conf)\n",
    "        U_values_stats_naive[conf] = (mean_U_n, ci_low_U_n, ci_high_U_n)\n",
    "\n",
    "        mean_utilization_Q1_n, ci_low_utilization_Q1_n, ci_high_utilization_Q1_n = naive_confidence_interval(queueLength_Q1 / np.sum(queueLength_Q1), confidence=conf)\n",
    "        utilization_Q1_stats_naive[conf] = (mean_utilization_Q1_n, ci_low_utilization_Q1_n, ci_high_utilization_Q1_n)\n",
    "\n",
    "        mean_utilization_Q2_n, ci_low_utilization_Q2_n, ci_high_utilization_Q2_n = naive_confidence_interval(queueLength_Q2 / np.sum(queueLength_Q2), confidence=conf)\n",
    "        utilization_Q2_stats_naive[conf] = (mean_utilization_Q2_n, ci_low_utilization_Q2_n, ci_high_utilization_Q2_n)\n",
    "\n",
    "        # Calcoli normale\n",
    "        mean_W_norm, ci_low_W_norm, ci_high_W_norm = normal_confidence_interval(lifeTime, confidence=conf)\n",
    "        lifeTime_stats_normal[conf] = (mean_W_norm, ci_low_W_norm, ci_high_W_norm)\n",
    "        \n",
    "        mean_U_norm, ci_low_U_norm, ci_high_U_norm = normal_confidence_interval(U_values, confidence=conf)\n",
    "        U_values_stats_normal[conf] = (mean_U_norm, ci_low_U_norm, ci_high_U_norm)\n",
    "\n",
    "        mean_utilization_Q1_norm, ci_low_utilization_Q1_norm, ci_high_utilization_Q1_norm = normal_confidence_interval(queueLength_Q1 / np.sum(queueLength_Q1), confidence=conf)\n",
    "        utilization_Q1_stats_normal[conf] = (mean_utilization_Q1_norm, ci_low_utilization_Q1_norm, ci_high_utilization_Q1_norm)\n",
    "\n",
    "        mean_utilization_Q2_norm, ci_low_utilization_Q2_norm, ci_high_utilization_Q2_norm = normal_confidence_interval(queueLength_Q2 / np.sum(queueLength_Q2), confidence=conf)\n",
    "        utilization_Q2_stats_normal[conf] = (mean_utilization_Q2_norm, ci_low_utilization_Q2_norm, ci_high_utilization_Q2_norm)\n",
    "\n",
    "    max_W = np.max(lifeTime)\n",
    "    min_W = np.min(lifeTime)\n",
    "\n",
    "    results.append({\n",
    "        \"File\": remove_vec_suffix(os.path.basename(file_path)),\n",
    "        \"File_Number\": get_file_number(remove_vec_suffix(os.path.basename(file_path))),\n",
    "        \"Description\": description,\n",
    "        \"max_W\": max_W,\n",
    "        \"min_W\": min_W,\n",
    "        **{f\"mean_W_t_student_{int(conf*100)}\": lifeTime_stats_t_student[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_W_t_student_{int(conf*100)}\": lifeTime_stats_t_student[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_W_t_student_{int(conf*100)}\": lifeTime_stats_t_student[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_U_t_student_{int(conf*100)}\": U_values_stats_t_student[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_U_t_student_{int(conf*100)}\": U_values_stats_t_student[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_U_t_student_{int(conf*100)}\": U_values_stats_t_student[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_utilization_Q1_t_student_{int(conf*100)}\": utilization_Q1_stats_t_student[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_utilization_Q1_t_student_{int(conf*100)}\": utilization_Q1_stats_t_student[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_utilization_Q1_t_student_{int(conf*100)}\": utilization_Q1_stats_t_student[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_utilization_Q2_t_student_{int(conf*100)}\": utilization_Q2_stats_t_student[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_utilization_Q2_t_student_{int(conf*100)}\": utilization_Q2_stats_t_student[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_utilization_Q2_t_student_{int(conf*100)}\": utilization_Q2_stats_t_student[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_W_naive_{int(conf*100)}\": lifeTime_stats_naive[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_W_naive_{int(conf*100)}\": lifeTime_stats_naive[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_W_naive_{int(conf*100)}\": lifeTime_stats_naive[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_U_naive_{int(conf*100)}\": U_values_stats_naive[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_U_naive_{int(conf*100)}\": U_values_stats_naive[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_U_naive_{int(conf*100)}\": U_values_stats_naive[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_utilization_Q1_naive_{int(conf*100)}\": utilization_Q1_stats_naive[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_utilization_Q1_naive_{int(conf*100)}\": utilization_Q1_stats_naive[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_utilization_Q1_naive_{int(conf*100)}\": utilization_Q1_stats_naive[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_utilization_Q2_naive_{int(conf*100)}\": utilization_Q2_stats_naive[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_utilization_Q2_naive_{int(conf*100)}\": utilization_Q2_stats_naive[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_utilization_Q2_naive_{int(conf*100)}\": utilization_Q2_stats_naive[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_W_normal_{int(conf*100)}\": lifeTime_stats_normal[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_W_normal_{int(conf*100)}\": lifeTime_stats_normal[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_W_normal_{int(conf*100)}\": lifeTime_stats_normal[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_U_normal_{int(conf*100)}\": U_values_stats_normal[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_U_normal_{int(conf*100)}\": U_values_stats_normal[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_U_normal_{int(conf*100)}\": U_values_stats_normal[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_utilization_Q1_normal_{int(conf*100)}\": utilization_Q1_stats_normal[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_utilization_Q1_normal_{int(conf*100)}\": utilization_Q1_stats_normal[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_utilization_Q1_normal_{int(conf*100)}\": utilization_Q1_stats_normal[conf][2] for conf in conf_levels},\n",
    "        **{f\"mean_utilization_Q2_normal_{int(conf*100)}\": utilization_Q2_stats_normal[conf][0] for conf in conf_levels},\n",
    "        **{f\"ci_low_utilization_Q2_normal_{int(conf*100)}\": utilization_Q2_stats_normal[conf][1] for conf in conf_levels},\n",
    "        **{f\"ci_high_utilization_Q2_normal_{int(conf*100)}\": utilization_Q2_stats_normal[conf][2] for conf in conf_levels},\n",
    "        \"mean_queueLength_Q1\": np.mean(queueLength_Q1),\n",
    "        \"mean_queueLength_Q2\": np.mean(queueLength_Q2),\n",
    "        \"lifeTime\": \",\".join(map(str, lifeTime)),\n",
    "        \"lifeTime_arrival\": \",\".join(map(str, lifeTime_arrival)),\n",
    "        \"queueLength_Q1\": \",\".join(map(str, queueLength_Q1)),\n",
    "        \"queueLength_Q2\": \",\".join(map(str, queueLength_Q2)),\n",
    "        \"lifeTime_change_Q1\": \",\".join(map(str, lifeTime_change_Q1)),\n",
    "        \"lifeTime_change_Q2\": \",\".join(map(str, lifeTime_change_Q2))\n",
    "    })\n",
    "\n",
    "# Funzione per analizzare tutti i file CSV in una directory\n",
    "def analyze_directory(directory_path):\n",
    "    ignored_files = []\n",
    "    results = []\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.csv') and not file_name.endswith('_sca.csv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            analyze_csv(file_path, ignored_files, results)\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df.sort_values(by='File_Number')  # Ordina per numero del file\n",
    "        results_df.drop(columns=['File_Number'], inplace=True)  # Rimuovi la colonna File_Number\n",
    "        results_df.to_csv(os.path.join('./results_summary_convalidation_2.csv'), index=False)\n",
    "    \n",
    "    if ignored_files:\n",
    "        ignored_files_sorted = sorted(ignored_files, key=lambda x: get_file_number(x[0]))  # Ordina gli ignorati per numero del file\n",
    "        ignored_df = pd.DataFrame(ignored_files_sorted, columns=['File', 'Description'])\n",
    "        ignored_df.to_csv(os.path.join('./ignored_files_convalidation_2.csv'), index=False)\n",
    "\n",
    "# Esempio di utilizzo\n",
    "directory_path = './results_CSV_convalidation_2'\n",
    "analyze_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "# Funzione per estrarre il valore di lambda dalla descrizione\n",
    "def extract_lambda(description):\n",
    "    match = re.search(r'lambda=([\\d\\.]+)', description)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Funzione per estrarre i valori di m1 e m2 dalla descrizione\n",
    "def extract_m_values(description):\n",
    "    match_m1 = re.search(r'm1=([\\d\\.]+)', description)\n",
    "    match_m2 = re.search(r'm2=([\\d\\.]+)', description)\n",
    "    if match_m1 and match_m2:\n",
    "        return float(match_m1.group(1)), float(match_m2.group(1))\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Funzione per convertire la stringa di tempi in una lista di float\n",
    "def convert_lifetime(lifetime_str):\n",
    "    return list(map(float, lifetime_str.split(',')))\n",
    "\n",
    "# Carica i dati da results_summary_convalidation.csv\n",
    "results_df = pd.read_csv('results_summary_convalidation_2.csv')\n",
    "\n",
    "# Determina la strategia dalla descrizione\n",
    "results_df['Strategy'] = results_df['Description'].apply(lambda desc: 'Exact_N' if 'Strategy=0' in desc else 'Limited_N')\n",
    "\n",
    "# Estrai i valori di lambda, m1 e m2 dalla descrizione\n",
    "results_df['lambda'] = results_df['Description'].apply(extract_lambda)\n",
    "results_df[['m1', 'm2']] = results_df['Description'].apply(lambda desc: pd.Series(extract_m_values(desc)))\n",
    "\n",
    "# Converte le stringhe di lifetime in liste di float\n",
    "results_df['lifeTime'] = results_df['lifeTime'].apply(convert_lifetime)\n",
    "\n",
    "# Calcola le colonne L osservato e L teorico usando le medie delle lunghezze delle code\n",
    "results_df['L_observed'] = results_df['mean_queueLength_Q1'] + results_df['mean_queueLength_Q2']\n",
    "results_df['L_theoretical'] = results_df['lambda'] * results_df['mean_W']\n",
    "results_df['L_theoretical_low'] = results_df['lambda'] * results_df['ci_low_W']\n",
    "results_df['L_theoretical_high'] = results_df['lambda'] * results_df['ci_high_W']\n",
    "results_df['L_difference'] = abs(results_df['L_observed'] - results_df['L_theoretical'])\n",
    "results_df['percentage_L_difference'] = (results_df['L_difference'] / results_df['L_observed']) * 100\n",
    "\n",
    "# Calcola L_low e L_max per il teorema di Little\n",
    "results_df['little_verification'] = results_df.apply(lambda row: row['L_theoretical_low'] <= row['L_observed'] <= row['L_theoretical_high'], axis=1)\n",
    "\n",
    "# Salva i risultati in un file CSV\n",
    "results_df.to_csv('LittleLaw_results.csv', index=False)\n",
    "\n",
    "# Mostra i risultati per debug\n",
    "print(results_df[['File', 'Strategy', 'lambda', 'm1', 'm2', 'mean_W', 'ci_low_W', 'ci_high_W', 'L_observed', 'L_theoretical', 'L_theoretical_low', 'L_theoretical_high', 'L_difference', 'percentage_L_difference', 'little_verification']])\n",
    "\n",
    "# Grafico per verificare il teorema di Little per tutte le configurazioni con politiche Limited_N ed Exact_N\n",
    "strategies = ['Exact_N', 'Limited_N']\n",
    "lambda_colors = {1.0: 'blue', 1.25: 'red'}\n",
    "\n",
    "for strategy in strategies:\n",
    "    subset = results_df[results_df['Strategy'] == strategy]\n",
    "    if not subset.empty:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for lambda_value, color in lambda_colors.items():\n",
    "            subsubset = subset[subset['lambda'] == lambda_value]\n",
    "            if not subsubset.empty:\n",
    "                x_values = subsubset.apply(lambda row: f\"m1={row['m1']}\\nm2={row['m2']}\", axis=1)\n",
    "                plt.fill_between(np.arange(len(x_values)), subsubset['L_theoretical_low'], subsubset['L_theoretical_high'], color=color, alpha=0.2, label=f'Confidence Interval (lambda={lambda_value})')\n",
    "                plt.plot(np.arange(len(x_values)), subsubset['L_observed'], marker='o', linestyle='-', color=color, label=f'L observed (lambda={lambda_value})')\n",
    "                plt.plot(np.arange(len(x_values)), subsubset['L_theoretical'], marker='x', linestyle='--', color=color, label=f'L theoretical (lambda={lambda_value})')\n",
    "        plt.xticks(np.arange(len(x_values)), x_values, rotation=90)\n",
    "        plt.xlabel('m1 and m2 values')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title(f'Verification of Little\\'s Law for Strategy: {strategy}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.gca().yaxis.set_major_formatter(ScalarFormatter(useOffset=False))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Convalidation_2_Images/LittleLaw_{strategy}.png')\n",
    "        plt.savefig(f'Convalidation_2_Images/LittleLaw_{strategy}.eps')  \n",
    "        plt.show()\n",
    "\n",
    "# Grafico della presenza media di job nel sistema per le due code\n",
    "for strategy in strategies:\n",
    "    subset = results_df[results_df['Strategy'] == strategy]\n",
    "    if not subset.empty:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        for lambda_value, color in lambda_colors.items():\n",
    "            subsubset = subset[subset['lambda'] == lambda_value]\n",
    "            if not subsubset.empty:\n",
    "                x_values = subsubset.apply(lambda row: f\"m1={row['m1']}\\nm2={row['m2']}\", axis=1)\n",
    "                \n",
    "                ax1.plot(np.arange(len(x_values)), subsubset['mean_queueLength_Q1'], marker='o', linestyle='-', color=color, label=f'Mean Queue Length Q1 (lambda={lambda_value})')\n",
    "                ax1.set_xticks(np.arange(len(x_values)))\n",
    "                ax1.set_xticklabels(x_values, rotation=90)\n",
    "                ax1.set_xlabel('m1 and m2 values')\n",
    "                ax1.set_ylabel('Mean Queue Length Q1')\n",
    "                ax1.set_title('Mean Queue Length Q1')\n",
    "                ax1.legend()\n",
    "                ax1.grid(True)\n",
    "                ax1.yaxis.set_major_formatter(ScalarFormatter(useOffset=False))\n",
    "                \n",
    "                ax2.plot(np.arange(len(x_values)), subsubset['mean_queueLength_Q2'], marker='x', linestyle='-', color=color, label=f'Mean Queue Length Q2 (lambda={lambda_value})')\n",
    "                ax2.set_xticks(np.arange(len(x_values)))\n",
    "                ax2.set_xticklabels(x_values, rotation=90)\n",
    "                ax2.set_xlabel('m1 and m2 values')\n",
    "                ax2.set_ylabel('Mean Queue Length Q2')\n",
    "                ax2.set_title('Mean Queue Length Q2')\n",
    "                ax2.legend()\n",
    "                ax2.grid(True)\n",
    "                ax2.yaxis.set_major_formatter(ScalarFormatter(useOffset=False))\n",
    "\n",
    "        fig.suptitle(f'Mean Job Presence for Strategy: {strategy}')\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the main title\n",
    "        plt.savefig(f'Convalidation_2_Images/Mean_Job_Presence_{strategy}.png')\n",
    "        plt.savefig(f'Convalidation_2_Images/Mean_Job_Presence_{strategy}.eps')  \n",
    "        plt.show()\n",
    "\n",
    "# Calcolo degli utenti medi per lambda 1.0 e 1.25\n",
    "mean_users_lambda_1_0 = results_df[results_df['lambda'] == 1.0]['L_observed'].mean()\n",
    "mean_users_lambda_1_25 = results_df[results_df['lambda'] == 1.25]['L_observed'].mean()\n",
    "\n",
    "print(f\"Utenti medi nel sistema per lambda=1.0: {mean_users_lambda_1_0}\")\n",
    "print(f\"Utenti medi nel sistema per lambda=1.25: {mean_users_lambda_1_25}\")\n",
    "\n",
    "# Calcolo della presenza media in Q1 e Q2 globalmente\n",
    "mean_presence_Q1 = results_df['mean_queueLength_Q1'].mean()\n",
    "mean_presence_Q2 = results_df['mean_queueLength_Q2'].mean()\n",
    "\n",
    "# Calcolo della presenza media in Q1 e Q2 per lambda 1.0 e 1.25\n",
    "mean_presence_Q1_lambda_1_0 = results_df[results_df['lambda'] == 1.0]['mean_queueLength_Q1'].mean()\n",
    "mean_presence_Q2_lambda_1_0 = results_df[results_df['lambda'] == 1.0]['mean_queueLength_Q2'].mean()\n",
    "mean_presence_Q1_lambda_1_25 = results_df[results_df['lambda'] == 1.25]['mean_queueLength_Q1'].mean()\n",
    "mean_presence_Q2_lambda_1_25 = results_df[results_df['lambda'] == 1.25]['mean_queueLength_Q2'].mean()\n",
    "\n",
    "print(f\"Presenza media in Q1: {mean_presence_Q1}\")\n",
    "print(f\"Presenza media in Q2: {mean_presence_Q2}\")\n",
    "print(f\"Presenza media in Q1 per lambda=1.0: {mean_presence_Q1_lambda_1_0}\")\n",
    "print(f\"Presenza media in Q2 per lambda=1.0: {mean_presence_Q2_lambda_1_0}\")\n",
    "print(f\"Presenza media in Q1 per lambda=1.25: {mean_presence_Q1_lambda_1_25}\")\n",
    "print(f\"Presenza media in Q2 per lambda=1.25: {mean_presence_Q2_lambda_1_25}\")\n",
    "\n",
    "# Calcola gli utenti medi nel sistema\n",
    "mean_users = results_df['L_observed'].mean()\n",
    "print(f\"Utenti medi nel sistema: {mean_users}\")\n",
    "\n",
    "# Configurazioni che hanno passato il teorema di Little\n",
    "little_law_passed = results_df[results_df['little_verification']]\n",
    "\n",
    "print(\"Configurazioni che hanno passato il teorema di Little:\")\n",
    "print(little_law_passed[['File', 'Strategy', 'lambda', 'm1', 'm2']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Funzione per estrarre il valore di lambda dalla descrizione\n",
    "def extract_lambda(description):\n",
    "    match = re.search(r'lambda=([\\d\\.]+)', description)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Funzione per estrarre i valori di m1 e m2 dalla descrizione\n",
    "def extract_m_values(description):\n",
    "    match_m1 = re.search(r'm1=([\\d\\.]+)', description)\n",
    "    match_m2 = re.search(r'm2=([\\d\\.]+)', description)\n",
    "    if match_m1 and match_m2:\n",
    "        return float(match_m1.group(1)), float(match_m2.group(1))\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Funzione per convertire la stringa di tempi in una lista di float\n",
    "def convert_lifetime(lifetime_str):\n",
    "    return list(map(float, lifetime_str.split(',')))\n",
    "\n",
    "# Funzione per calcolare l'intervallo di confidenza con distribuzione normale\n",
    "def normal_confidence_interval(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    std_error = stats.sem(data)\n",
    "    z_value = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
    "    margin = z_value * std_error\n",
    "    return mean, mean - margin, mean + margin\n",
    "\n",
    "# Carica i dati da results_summary_convalidation.csv\n",
    "results_df = pd.read_csv('results_summary_convalidation_2.csv')\n",
    "\n",
    "# Determina la strategia dalla descrizione\n",
    "results_df['Strategy'] = results_df['Description'].apply(lambda desc: 'Exact_N' if 'Strategy=0' in desc else 'Limited_N')\n",
    "\n",
    "# Filtra solo le configurazioni per N_Limited\n",
    "results_df = results_df[results_df['Strategy'] == 'Limited_N']\n",
    "\n",
    "# Estrai i valori di lambda, m1 e m2 dalla descrizione\n",
    "results_df['lambda'] = results_df['Description'].apply(extract_lambda)\n",
    "results_df[['m1', 'm2']] = results_df['Description'].apply(lambda desc: pd.Series(extract_m_values(desc)))\n",
    "\n",
    "# Converte le stringhe di lifetime in liste di float\n",
    "results_df['lifeTime'] = results_df['lifeTime'].apply(convert_lifetime)\n",
    "\n",
    "# Calcola le colonne L osservato\n",
    "results_df['L_observed'] = results_df['mean_queueLength_Q1'] + results_df['mean_queueLength_Q2']\n",
    "\n",
    "# Funzione per creare le tabelle di verifica del teorema di Little per ogni intervallo di confidenza\n",
    "def create_little_law_table(df, conf_label, method):\n",
    "    if method == 't_student':\n",
    "        mean_col = f'mean_W_t_student_{conf_label}'\n",
    "        low_col = f'ci_low_W_t_student_{conf_label}'\n",
    "        high_col = f'ci_high_W_t_student_{conf_label}'\n",
    "    elif method == 'naive':\n",
    "        mean_col = f'mean_W_naive_{conf_label}'\n",
    "        low_col = f'ci_low_W_naive_{conf_label}'\n",
    "        high_col = f'ci_high_W_naive_{conf_label}'\n",
    "    elif method == 'normal':\n",
    "        mean_col = f'mean_W_normal_{conf_label}'\n",
    "        low_col = f'ci_low_W_normal_{conf_label}'\n",
    "        high_col = f'ci_high_W_normal_{conf_label}'\n",
    "    \n",
    "    table_df = df[['File', 'lambda', 'm1', 'm2', mean_col, low_col, high_col, 'L_observed']]\n",
    "    table_df = table_df.rename(columns={\n",
    "        mean_col: f'L_theoretical_{conf_label}',\n",
    "        low_col: f'L_theoretical_low_{conf_label}',\n",
    "        high_col: f'L_theoretical_high_{conf_label}'\n",
    "    })\n",
    "    table_df[f'L_theoretical_{conf_label}'] = table_df['lambda'] * table_df[f'L_theoretical_{conf_label}']\n",
    "    table_df[f'L_theoretical_low_{conf_label}'] = table_df['lambda'] * table_df[f'L_theoretical_low_{conf_label}']\n",
    "    table_df[f'L_theoretical_high_{conf_label}'] = table_df['lambda'] * table_df[f'L_theoretical_high_{conf_label}']\n",
    "    table_df['little_verification'] = table_df.apply(lambda row: row[f'L_theoretical_low_{conf_label}'] <= row['L_observed'] <= row[f'L_theoretical_high_{conf_label}'], axis=1)\n",
    "    table_df['Verification'] = table_df['little_verification'].apply(lambda x: 'X' if x else '')\n",
    "    return table_df[['File', 'lambda', 'm1', 'm2', f'L_theoretical_{conf_label}', 'L_observed', f'L_theoretical_low_{conf_label}', f'L_theoretical_high_{conf_label}', 'Verification']]\n",
    "\n",
    "# Crea le tabelle per ogni intervallo di confidenza per tutti i metodi\n",
    "conf_levels = [95, 90, 85, 80]\n",
    "tables_t_student = {}\n",
    "tables_naive = {}\n",
    "tables_normal = {}\n",
    "\n",
    "for conf in conf_levels:\n",
    "    conf_label = f'{conf}'\n",
    "    tables_t_student[conf_label] = create_little_law_table(results_df, conf_label, 't_student')\n",
    "    tables_naive[conf_label] = create_little_law_table(results_df, conf_label, 'naive')\n",
    "    tables_normal[conf_label] = create_little_law_table(results_df, conf_label, 'normal')\n",
    "\n",
    "# Salva ogni tabella in un file CSV separato per tutti i metodi\n",
    "for conf_label, table in tables_t_student.items():\n",
    "    table.to_csv(f'LittleLaw_verification_table_t_student_{conf_label}.csv', index=False)\n",
    "    print(f\"\\nTabella per l'intervallo di confidenza {conf_label}% (t-student):\")\n",
    "    print(table)\n",
    "\n",
    "for conf_label, table in tables_naive.items():\n",
    "    table.to_csv(f'LittleLaw_verification_table_naive_{conf_label}.csv', index=False)\n",
    "    print(f\"\\nTabella per l'intervallo di confidenza {conf_label}% (naive):\")\n",
    "    print(table)\n",
    "\n",
    "for conf_label, table in tables_normal.items():\n",
    "    table.to_csv(f'LittleLaw_verification_table_normal_{conf_label}.csv', index=False)\n",
    "    print(f\"\\nTabella per l'intervallo di confidenza {conf_label}% (normale):\")\n",
    "    print(table)\n",
    "\n",
    "# Aggiungi il calcolo dell'intervallo di confidenza con distribuzione normale ai risultati\n",
    "for conf in conf_levels:\n",
    "    conf_label = f'{conf}'\n",
    "    results_df[f'mean_W_normal_{conf_label}'] = results_df['lifeTime'].apply(lambda x: normal_confidence_interval(x, confidence=conf/100)[0])\n",
    "    results_df[f'ci_low_W_normal_{conf_label}'] = results_df['lifeTime'].apply(lambda x: normal_confidence_interval(x, confidence=conf/100)[1])\n",
    "    results_df[f'ci_high_W_normal_{conf_label}'] = results_df['lifeTime'].apply(lambda x: normal_confidence_interval(x, confidence=conf/100)[2])\n",
    "\n",
    "# Salva il DataFrame con i nuovi calcoli\n",
    "results_df.to_csv('results_summary_convalidation_2_with_normal.csv', index=False)\n",
    "\n",
    "# Esempio di utilizzo\n",
    "directory_path = './results_CSV_convalidation_2'\n",
    "analyze_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Funzione per troncare i valori al terzo decimale e salvare le colonne richieste\n",
    "def truncate_and_save_csv(file_path):\n",
    "    # Leggi il file CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Seleziona le colonne dalla 5 alla 9 (0-indexed, quindi dalla colonna 4 alla colonna 8)\n",
    "    df_selected = df.iloc[:, 4:9]\n",
    "    \n",
    "    # Tronca i valori al terzo decimale\n",
    "    df_truncated = df_selected.round(3)\n",
    "    \n",
    "    # Genera il nuovo nome del file\n",
    "    new_file_path = file_path.replace('.csv', '_truncated.csv')\n",
    "    \n",
    "    # Salva il nuovo file CSV\n",
    "    df_truncated.to_csv(new_file_path, index=False)\n",
    "    \n",
    "    return new_file_path\n",
    "\n",
    "# Funzione per elaborare tutti i file CSV di verifica del teorema di Little\n",
    "def process_little_law_verification_files():\n",
    "    conf_levels = [95, 90, 85, 80]\n",
    "    methods = ['t_student', 'naive', 'normal']\n",
    "    \n",
    "    for method in methods:\n",
    "        for conf in conf_levels:\n",
    "            conf_label = f'{conf}'\n",
    "            file_path = f'LittleLaw_verification_table_{method}_{conf_label}.csv'\n",
    "            new_file_path = truncate_and_save_csv(file_path)\n",
    "            print(f\"Generato file troncato: {new_file_path}\")\n",
    "\n",
    "# Esegui l'elaborazione dei file\n",
    "process_little_law_verification_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leggi i dati dal file CSV\n",
    "df = pd.read_csv('./results_summary_convalidation_2_with_normal.csv')\n",
    "\n",
    "# Filtro le righe che nel campo 'Description' contengono 'Strategy=1'\n",
    "strategy_1_df = df[df['Description'].str.contains('Strategy=1', case=False, na=False)]\n",
    "\n",
    "# Trova tutte le colonne che contengono 'mean_W' nel loro nome\n",
    "mean_W_cols = [col for col in df.columns if 'mean_W' in col]\n",
    "\n",
    "# Calcola la media di queste colonne per ogni riga filtrata\n",
    "strategy_1_df['mean_W_row'] = strategy_1_df[mean_W_cols].mean(axis=1)\n",
    "\n",
    "# Calcola la media globale di queste medie\n",
    "mean_W_global = strategy_1_df['mean_W_row'].mean()\n",
    "\n",
    "print(\"Mean W globale per Strategy = N_Limited:\")\n",
    "print(mean_W_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carica il DataFrame dal file CSV\n",
    "df = pd.read_csv('./results_summary_convalidation_2.csv')\n",
    "\n",
    "# Filtra il DataFrame per considerare solo le simulazioni con \"Strategy=1\" nel campo \"Description\"\n",
    "df = df[df[\"Description\"].str.contains(\"Strategy=1\")]\n",
    "\n",
    "# Converti le colonne in liste di float\n",
    "df[\"queueLength_Q1\"] = df[\"queueLength_Q1\"].apply(lambda x: list(map(lambda y: int(float(y)), x.split(','))))\n",
    "df[\"queueLength_Q2\"] = df[\"queueLength_Q2\"].apply(lambda x: list(map(lambda y: int(float(y)), x.split(','))))\n",
    "df[\"lifeTime_change_Q1\"] = df[\"lifeTime_change_Q1\"].apply(lambda x: list(map(float, x.split(','))))\n",
    "df[\"lifeTime_change_Q2\"] = df[\"lifeTime_change_Q2\"].apply(lambda x: list(map(float, x.split(','))))\n",
    "\n",
    "# Dizionario per accumulare i valori di lt_differences\n",
    "result_dict = {}\n",
    "num_simulations = len(df)\n",
    "\n",
    "# Itera su tutte le righe del DataFrame filtrato\n",
    "for index, row in df.iterrows():\n",
    "    UsersQ1 = row[\"queueLength_Q1\"]\n",
    "    UsersQ2 = row[\"queueLength_Q2\"]\n",
    "    TimesQ1 = row[\"lifeTime_change_Q1\"]\n",
    "    TimesQ2 = row[\"lifeTime_change_Q2\"]\n",
    "\n",
    "    # Funzione per ottenere il numero di utenti in un determinato momento\n",
    "    def get_user_count(times, users, time):\n",
    "        for i, t in enumerate(times):\n",
    "            if t == time:\n",
    "                return users[i]\n",
    "            elif t > time:\n",
    "                return users[i-1]\n",
    "        return users[-1]\n",
    "\n",
    "    # Combiniamo e ordiniamo i tempi\n",
    "    combined_times = sorted(set(TimesQ1 + TimesQ2))\n",
    "\n",
    "    # Calcoliamo i valori combinati degli utenti\n",
    "    UsersQ1_Q2 = []\n",
    "    for time in combined_times:\n",
    "        users_q1 = get_user_count(TimesQ1, UsersQ1, time)\n",
    "        users_q2 = get_user_count(TimesQ2, UsersQ2, time)\n",
    "        UsersQ1_Q2.append(users_q1 + users_q2)\n",
    "\n",
    "    # Calcolo delle differenze di tempo e aggiornamento del dizionario\n",
    "    for i in range(1, len(combined_times)):\n",
    "        lt_difference = combined_times[i] - combined_times[i-1]\n",
    "        key = UsersQ1_Q2[i-1]\n",
    "        if key not in result_dict:\n",
    "            result_dict[key] = 0\n",
    "        result_dict[key] += lt_difference\n",
    "\n",
    "# Dividi i valori accumulati per il numero di simulazioni\n",
    "for key in result_dict:\n",
    "    result_dict[key] /= num_simulations\n",
    "\n",
    "# Creazione del nuovo DataFrame\n",
    "new_df = pd.DataFrame(list(result_dict.items()), columns=[\"agents_in_the_system\", \"lt_difference\"])\n",
    "\n",
    "# Funzione per stampare la somma totale di lt_differences\n",
    "def print_total_lt_difference(df):\n",
    "    total_lt_difference = df[\"lt_difference\"].sum()\n",
    "    print(f\"La somma totale di tutti i lt_differences è: {total_lt_difference}\")\n",
    "\n",
    "# Utilizzo della funzione\n",
    "print_total_lt_difference(new_df)\n",
    "\n",
    "# Salvataggio del DataFrame in un file CSV\n",
    "new_df.to_csv(\"users_times.csv\", index=False)\n",
    "\n",
    "# Visualizzazione del nuovo DataFrame\n",
    "print(new_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inifile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
